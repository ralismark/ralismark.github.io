---
layout: post
title: Behavioural Ratchets
excerpt: You don't need others to race to the bottom
date: 2022-04-28
tags:
---

# I

The FDA is the American institution responsible for approving drugs.
However, its main _motivation_ isn't to directly save lives, but instead to minimise how much it can be blamed -- Scott Alexander has written [numerous][fda1] [posts][fda2] [about][fda3] [this][fda4].
Each of those four links is a full-blown story about the FDA being too cautious to the detriment of public health.

[fda1]: https://astralcodexten.substack.com/p/details-of-the-infant-fish-oil-story
[fda2]: https://astralcodexten.substack.com/p/adumbrations-of-aducanumab
[fda3]: https://astralcodexten.substack.com/p/the-fda-has-punted-decisions-about
[fda4]: https://astralcodexten.substack.com/p/when-will-the-fda-approve-paxlovid

If a drug gets approved and goes on to have issues (which, seemingly, can be as mild as "the drug might not work", as in the case of [aducanumab][fda2]), there's outcry and posts criticising the FDA for being lax, but pretty much the only thing that happens when something _doesn't_ get approval is a blog post from a [particular psychiatrist][acx].

[acx]: https://astralcodexten.substack.com/

# II

From [a somewhat recent ACX post](https://astralcodexten.substack.com/p/heuristics-that-almost-always-work):

*[The Queen] rules over a volcanic island. Everyone worries when the volcano would erupt. The wisest men of the kingdom research the problem and decide that the volcano has a straight 1/1000 chance of erupting any given year, uncorrelated with whether it erupted the year before. There are some telltale signs legible to the wise - a slight change in the color of the lava, an imperceptible shift in the smell of the sulfur - but nothing obvious until it's too late.*

*The queen founded a Learned Society Of Vulcanologists and charged them with predicting when the volcano will erupt. Unbeknownst to her, there were two kinds of vulcanologists. Honest vulcanologists, who genuinely tried to read the signs as best they can. And The Cult Of The Rock, an evil sect who gained diabolical knowledge by communing in secret with a rock containing the words "THE VOLCANO IS NOT ERUPTING".*

*Every so often an honest vulcanologist felt like the lava was starting to look little weird and told the Queen. The Queen panicked and ask everyone for advice. The Honest vulcanologists said "look, it's a hard question, the lava seems kind of weird today but it's always weird in some way or other, this volcano rarely erupts but for all we know this time might be the exception". The rock cultists secretly checked their rock and said "No, don't worry, the volcano is not erupting". Then the volcano didn't erupt. The Queen punished the trigger-happy vulcanologist who sounded the false alarm, grumbled at the useless vulcanologists who weren't sure either way, and promoted the confident cultists who correctly predicted everything was okay.*

*Time passed. With each passing year, the cultists and the institutions and methods of thought that produced them gained more and more status relative to the honest vulcanologists and their institutions and methods. The Queen died, her successor succeeded, and the island kept going along the same lines for let's say five hundred years.*

*After five hundred years, the lava looked a bit weird, and the new Queen consulted her advisors. By this time they were 100% cultists, so they all consulted the rock and said "No, the volcano is not erupting". The sulfur started to smell different, and the Queen asked "Are you sure?" and they double-checked the rock and said "Yeah, we're sure". The earth started to shake, and the Queen asked them one last time, so they got tiny magnifying glassses and looked at the rock as closely as they could, but it still said "THE VOLCANO IS NOT ERUPTING". Then the volcano erupted and everyone died. The end.*

# III

I'm quoting ACX a bit too much so here's a personal anecdote for a change.

Ever since finding LessWrong and going through the resources there, I've been acutely and likely excessively aware of how my own reasoning can go wrong.
It's honestly pretty tempting, given the catalogue of biases and reasoning mistakes you can even run into even if you're paying attention.
And at the time, my mental state was just right to get caught in this kind of trap.

And so it unfolds over the following months:
I notice instances where my judgement is wrong and adjust confidence in my judgement downwards.
I read an argument and its rebuttal, both sounding pretty reasonable themselves but completely invalid in light of the other[^learned-helplessness]; adjust downwards again.
And when I am correct, it's not noticeable -- unless you legitimately expect to fail, who gets surprised about _not_ messing up? -- and so nothing happens.
These instances keep showing up, gradually chipping away at my confidence in my own judgement without any opposing force to balance it, and eventually accumulating into the self-trust issues I have nowadays.

[^learned-helplessness]: Scott has a post about this actually: [Epistemic Learned Helplessness](https://slatestarcodex.com/2019/06/03/repost-epistemic-learned-helplessness/)

(The covid lockdowns when I was going through this didn't help either)

This kind of emphasis on your own shortfalls over your strengths is also a central part of [imposter syndrome] -- it's not that everyone is better than you, it's just that you don't notice when you're doing thing well.

[imposter syndrome]: https://en.wikipedia.org/wiki/Impostor_syndrome

.. figure:: {{ recipe.copy("./imposter-syndrome.png", "/assets/imposter-syndrome.png") }}
	:alt: Two sets of circles.
		The left group is labelled "Imposter Syndrome" and shows a small blue circle with "What I know", completely contained inside a larger yellow circle "What I think others know".
		The right group, labelled "Reality", consists of a central blue circle "What I know" surrounded by and slightly overlapping yellow circles of the same size as blue one.
		These yellow circles are labelled "What others know"

	A diagram that shows up a lot in discussions surrounding imposter syndrome.

(The flipside of this is when you're consistently _overconfident_ and don't expect to encounter issues or delays -- [Optimism Bias] or the [Planning Fallacy].
And there's the recursive extension that is [Hofstadter's Law]: "It always takes longer than you expect, even when you take into account Hofstadter's Law")

[Optimism Bias]: https://en.wikipedia.org/wiki/Optimism_bias
[Planning Fallacy]: https://en.wikipedia.org/wiki/Planning_fallacy
[Hofstadter's Law]: https://en.wikipedia.org/wiki/Hofstadter's_law

# IV

A ratchet is a mechanical device consisting of a gear and a spring-loaded finger that only allows a gear to rotate in one direction.
In the forwards direction, the spring loaded finger slides over the teeth of the gear, allowing free movement, but movement in the reverse direction engages the finger and causes the gear to lock up.
They're used in mechanical clocks, cable ties (where the gear is replaced with a rack), and bicycles.

.. figure:: https://upload.wikimedia.org/wikipedia/commons/thumb/b/b7/Ratchet_Drawing.svg/1280px-Ratchet_Drawing.svg.png
	:alt: A diagram of a ratchet mechanism.
		There is a rotating gear with a mechanical finger catching the teeth of the gear.

	The finger slides over the gear when it rotates counterclockwise, and after each step locks it from rotating clockwise.
	(attribution: [^ratchet-attrib])

[^ratchet-attrib]: "[Ratchet Drawing][ratchet-attrib1]" by [Dr. Schorsch][ratchet-attrib2], licensed under [CC BY-SA 3.0][ratchet-attrib3]
[ratchet-attrib1]: https://commons.wikimedia.org/wiki/File:Ratchet_Drawing.svg
[ratchet-attrib2]: https://commons.wikimedia.org/wiki/User:Xorx
[ratchet-attrib3]: https://creativecommons.org/licenses/by-sa/3.0/legalcode

In the same way, behaviour in these situations naturally only adjusts in one direction.
The FDA ratchets towards caution and withholding treatments that could improve people's lives.
The Queen's dismissal of honest but mistaken vulcanologists ratchets the Learned Society towards ignoring all warning signs.
And I ratchet towards self-esteem issues and anxiety (I'm somewhat better now).

Broadly, races to the bottom are also an example of this except involving multiple parties ratcheting each other.
Each step you need to sacrifice something you value to remain ahead, and you can never reverse this decision or else risk being outcompeted.

(In writing this I could only think of negative examples, where the response you get in a system always leads you away from ideal behaviour. But there's likely positive ones out there where you are forced into desirable behaviour)

# V

Things get _really_ hairy once you involve the [Law of Total Expectation], which goes by several names including the Law of Iterated Expectations, the tower rule, and of course LW's own [Conservation of Expected Evidence].
It says that merely knowing you'll see feedback shouldn't cause your expectations to adjust.

[Conservation of Expected Evidence]: https://www.lesswrong.com/tag/conservation-of-expected-evidence
[Law of total expectation]: https://en.wikipedia.org/wiki/Law_of_total_expectation

<figure markdown="1">

$$
E(X) = \sum_i {E(X|Y_i) \cdot P(Y_i)}
$$

<figcaption>
Law of total expectation:
On average, you don't expect to update in any direction.
</figcaption>
</figure>

It turns out that a corollary of this, if you look at the equation the other way, is that you can derive what your current expectation should be, from the likelihood of different results and your anticipated updates to each.
So if you expect to feedback to ratchet forwards, you should immediately advance instead of waiting for what you expect.

And just you keep jumping forwards, racing downwards.

And you only stop when you hit some sort of equilibrium where this doesn't tell you to update.
If you're lucky, this isn't that far from where you started.
If you're not, you just pushed expectation to the most extreme it can be.

Congrats on racing all the way to the bottom.
Enjoy your stay!

# VI

That previous section was a dramatic, but that's kinda how it felt to me.
But it does lead to a question:
what can we do if ratcheting is not correct?

One conclusion you can draw from planning fallacy and in particular Hofstadter's Law is to have limits on how much you update -- no matter how much you bias yourself towards expecting things taking longer, there's always limit to this bias.
At the extreme this involves never updating our expectations despite it obviously being incorrect -- a ratchet can never get stuck advanced if it never moves forward!
This definitely violates some rules of good reasoning, the least of which is throwing the Law of Total Expectation out the window.

If we do want to be consistent however, then we need to look back at the Law of Total Expectation.
The crux of the issue is that we update when things go poorly, but not when things go well.
So lack of bad feedback should be treated as good feedback!

# Postscript

This post is quite the deviation from the technical posts that are a staple here.
It's the result of being inspired by great bloggers in the rat-sphere, most notably [Scott Alexander/ACX][acx], and a desire to write something and to write something _non-tech_.
And, well, here we are, with what's hopefully a good article.

I welcome criticism of what I've laid out here.
Please also give praise if you think this is good, lest I ratchet further into believing my writing sucks :)
